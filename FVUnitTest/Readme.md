# FVUnitTest

Unit testing for FVConsole and FVEngine made using the MSTest framework.  

## Welcome :)

Hello me (or whoever) from the future! Hopefully you read this readme before looking at the code, as this might deter you from going on a murderous rampage.

This was my first useful project written using C#/.NET/Visual Studio and first time doing unittests in this environment as well. As a result, this suite is truly a spaghettified abomination, but at least it works. More importantly, lessons are learned. 

Tests in this suite are loaded dynamically from the `./data/case` directory using a test loader that looks for directories satisfying (roughly) the pattern `./case/(calc|verify|create)/test_*`. The loader examines the contents and builds the test parameters into a `ddt-*.csv` file used by the test runner to parameterize tests for FVConsole's `calc`, `verify`, and `create` modes, respectively. 

The tests are all loaded by one loader class and then executed by another, and entirely by CSV strings. I recommend that for the existing suite, only add data files following the same pattern as the files that already exist. For future testing, create a new suite containing a unique loader/runner dedicated for each type of test and try to do it in an organized way. I actually cleaned this code up significantly while writing the README, so it should be much easier to follow the logic now (it turns out the old method would have broken trying to add anything new to the `./data/case` folder anyway - oops). 

By the way, this specific document is also the first markdown-based README made for PBS, so this whole document can be considered proof-of-concept (or sloppy demo) for future work. 

## Prerequisites

### Nuget Packages

Note: See the package.config file for each project to get the most up-to-date and accurate list, including version numbers. 

* CSVHelper
* CommandLineParser
* ILMerge (used to merge assemblies into fvconsole.exe)
* Netwonsoft.Json

Other Microsoft-provided packages may be required, but should be included with the standard VS2017 installation. 

All packages should be installed from the NuGet package manager.

### Python Packages

Python scripts are used for the test suite. The remote console script is not particularly necessary, but the `ftp_host.py` script is required for testing FTP functionality. 

The only requirement is currently pyftpdlib:

```
pip install pyftpdlib
```

This suite was tested using Python v3.5.2, but any installation >=3.0 should work.  

## Installing

1. Install the required NuGet packages.
2. Install Python >=v3.5.2
3. Install the required Python packages
4. Try to run the test suite. If it doesn't work, handle it. 

## Running the tests

Test Explorer -> Run All tests. Should work out of the box. The tests are organized (more like "filtered") into three main categories: Calc, Create, and Verify. Subcategories for command variants are included as well.

For all three test types, the test loader adds a variant with and without the `-v` (verbose) switch. This means each test case loaded will result in two tests being executed. 

**Note:** If the FTP tests fail, try killing python and re-running: `tskill.exe python`. On occasion, the FTP server process doesn't get killed properly and can linger for subsequent tests to connect to by mistake. This typically happens when debugging tests and aborting when the code is stopped instead of exiting gracefully. 

## Test Suite Design

Because parameterized tests cannot be normally added programmatically by MSTest, they are instead written into CSV files (see `DDT-XXX.csv` in the `data` root folder) before the test methods are loaded. Because the files are not evaluated until the test method is ready to be called, the cases can be loaded and run automatically. 

Note that the current working directory is set to the `test_case` folder for each test case discovered so that all files can be accessed without needing to calculate the appropriate relative filepath. This greatly simplifies creating each test case.  

All testing is (currently) unit tests. Unit testing is amazing: despite time wasted and setbacks learning & designing the (current, crappy) test suite, it has *already saved time* as I went back and added FTP support for all or most parts of the code.  

Test Data was generated by a python script to generate source files using the `lorem` package. Checksums for test data were generated using `hashlib.sha256(text).hexdigest()`. Aside from easily generating the data, this does double-duty of ensuring the FVConsole's checksum routine matches an available reference. A copy of the script is located in the `scripts` directory. Note that it contains absolute filepaths needed to get it to work on my computer, and will need to be modified to run locally. 

**Note:** The test runner will automatically clean test artifacts (output files), so they should not be present after running unless something truly horrid has gone wrong. Use the "debug tests" feature in VS to debug the test suite instead of attempting to debug by disabling the cleanup code.  

**Note:** The test runner is currently configured to run `FileVerifierConsole.exe`, not the ILMerged `fvconsole.exe`. In practice, this shouldn't make a difference. In theory, the ILMerge script should be updated to move the `fvconsole.exe` to a sensible location where it can be located easily from the test suite. 

**Note:** FTP is not tested by Create tests (or by Verify tests). The underlying FVEngine interface used by all three programs only exposes a single function for retrieving checksum: `GetChecksum(string uri)`. FTP access is therefore sufficiently handled by the Calc tests. 

### Calc

Tests loaded from the `./data/case/calc/` directory. Tests are defined by presence specific, pre-named data files:

* Standard test:
	* `in.txt`: File to calculate checksum for.
	* `expect.txt`: File containing expected checksum as the first line. Other lines are ignored. 
* FTP test:
	* Note: triggered by presence of `ftp.txt`.
	* Includes all files in standard test
	* `ftp.txt`: Contains FTP base URL for testing via FTP access.  

The files are used as follows:

1. `In.txt` is the input file. if (`ftp.txt` exists) -> prepend to input filename for command line. 
2. Run: `> FileVerifierConsole.exe calc {input_file}`
3. compare contents of `expect.txt` vs. Standard Output of above command. 

### Create

Tests loaded from the `./data/case/create/` directory, defined similarly as the Calc tests. Here, all tests start with `test_batch*` or `test_single*`, to define whether the tests will be run in single mode or batch mode.

* Batch (basic):
	* `in.json`: JSON input file specifying source and target files. 
	* `expect.json`: JSON output file expected from a successful command. Note that the checksum must match the actual checksum of the file, or the test will fail. 
	* Other: the folder must contain all files listed by `in.json`.
* Batch (append):
	* Note: triggered by presence of `out.json`.
	* Includes all files in basic batch test.
	* `out.json`: Contains additional arbitrary `"key":"value"` pairs expected to be present in the output file after the program runs and appends to the file. Note that when the test runner runs in batch append mode, the data is output to a temporary `out-append.json` file instead of `out.json` as in the basic test.
* Single (append):
	* Includes all files present in Batch (append) tests. 

Files used as follows:

1. `In.json` is used as the input file. 
2. All files specified by `in.json` must exist, and will have contents checksummed. 
3. If batch mode, append `-b` switch. 
4. If append mode, append `-a` switch. 
5. If single mode, append `-t "target.txt"`
6. Run (depending on mode, one of): 
	1. `> FileVerifierConsole.exe create -b -i {in} -o {out}`
	2. `> FileVerifierConsole.exe create -b -i {in} -o {out} -a`
	3. `> FileVerifierConsole.exe create -i {in} -t {target} -o {out} -a`
4. Compare JSON-parsed contents of `expect.json` vs `{out}`:
	1. Must be same length.
	2. Must have all of the same keys.
	3. Value for each key must match. 

### Verify

Tests loaded from the `./data/case/verify/` directory. Tests are either run in Batch mode or Single mode.

* Batch:
	* `in.json`: Contains a JSON object (not list!) of `"filename": "checksum"` key-value pairs. 
	* Other: All files specified by `in.json` must be present in the test case folder. 
* Single:
	* Note: triggered by presence of `checksum.txt`.
	* Includes all files in Batch test.
	* `checksum.txt`: Contains checksum of `in.json` file.
* Fail (Batch or single):
	* If `fail.txt` is present, the test is expected to Fail successfully (ExitCode: 1)  

Files used as follows:

1. Batch mode:  `> FileVerifierConsole.exe verify {in}`
2. Single mode: `> FileVerifierConsole.exe verify {in} -c {checksum}`    

The test is considered successful if the EXE returns 0, or if the EXE returns 1 for a Fail test. These are the exact numbers tested, which ensure that installation scripts can reliably check against these values when running. 

## Authors

* **Nathan Starkweather** - *Initial work* - [PurpleBooth](https://github.com/PurpleBooth)

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Me, for being pretty great
* My cat Tinkerbell, for moral support
* My friend ABearCat, for pointing out that our automation testing framework (or lack thereof) was 15-20 years out of date, and for being even more negative about the quality of our software than I am. 
